{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03556489",
   "metadata": {},
   "source": [
    "# Cluster based Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01b9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "corpus_path = \"news-corpus//\"\n",
    "article_paths = [os.path.join(corpus_path,p) for p in os.listdir(corpus_path)]\n",
    "\n",
    "doc_complete = []\n",
    "for path in article_paths:\n",
    "    with open(path, 'rb') as f:\n",
    "        doc_content = f.read().decode(errors='ignore')\n",
    "        doc_complete.append(doc_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68bcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(doc_complete)):\n",
    "    doc_complete[i] = re.sub(r'[^\\w\\s.]', '', doc_complete[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943876be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(doc_complete) >= 2:\n",
    "    doc_complete.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8811a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d44e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236c5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1]\n",
      "Cluster 1 Summary:\n",
      "union agriculture minister narendra singh tomar friday said government making effort boost production consumption millet speaking inaugurating millet experience centre mec dilli haat ina national capital cooperative nafed collaboration agriculture ministry established centre create awareness benefit millet encourage adoption among general public agriculture secretary manoj ahuja nafed md rajbir singh present event making effort enhance production yield processing consumption miller country tomar said minister highlighted multiple health benefit consuming millet nutritious tomar said millet climate resilient grown le water minimal use fertiliser pesticide said increase production millet also boost income farmer especially small marginal one tomar noted large number startup commendable job field millet rued importance millet plate food got reduced year acting upon india proposal supported country united nation general assembly declared year international year millet iym declaration positioned india forefront celebration government working mission mode champion millet crop good farmer environment consumer tomar highlighted yearlong celebration millet shree anna entail myriad activity aimed creating awareness around environmental health economic benefit millet farming ministryled initiative establishing consumeroriented millet experience centre would promote dietary benefit ancient grain also popularise millet shree anna nutritional powerhouse fit cooking variety dish like millet dosa millet pasta etc addition unique dining experience customer also purchase variety readytoeat readytocook product local millet startup experience centre ahuja highlighted governmentled initiative mainstreaming millet speaking collaboration nafed ministry said venture like millet experience centre would help widen horizon consumer actively looking healthier alternative bring visibility india robust milletbased startup community nafed md rajbir singh said centre unique concept help recognition immense potential millet shree anna versatile healthy grain added centre enable consumer enjoy expansive millet menu instore shopping experience featuring wide variety milletbased product developed homegrown startup one roof singh said milletsbased product also instrumental promoting healthy snacking among customer encouraging towards adopting healthier milletscentric diet india produce lakh tonne millet jowar bajra ragi sawan kangni cheena kodo kutki kuttu major millet.\n",
      "==========================\n",
      "[0, 1]\n",
      "Cluster 2 Summary:\n",
      "food regulator fssai friday said new case food business operator fbos primafacie found violation misleading advertisement claim fssai referred matter licensing authority concerned issue notice fbos withdrawing misleading claim scientifically substantiate claim fbos include manufacturer andor marketer nutraceutical product refined oil pulse flour millet product ghee etc order keep close tab claim advertisement made fbos product advertisement monitoring committee fssai reported fresh case found prima facie contravention provision food safety standard advertisement claim regulation regulator said statement regulation deceptive claim advertisement prohibited punishable offence f act food product scrutinised include various range product like health supplement organic product fastmoving consumer good fmcg product staple etc claim identified include various health claim product claim etc action including issuance notice concerned fbos referred concerned licensing authority issuance notice fbos withdrawing misleading claim scientifically substantiate statement said case unsatisfactory response fbo required withdraw claim modify per provision said regulation failing fbo penalised fine extending r lakh per food safety standard act apart stringent punishment like suspensioncancellation license etc case repeated offence total number case reported misleading advertisement claim last six month gone case action delinquent fbos shall also continue future fssai said regulator asked fbos strictly adhere provision food safety standard advertisement claim regulation desist making unscientific andor exaggerated claim advertisement promote product sale avoid enforcement action.\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Preprocess the text data\n",
    "\n",
    "# if len(doc_complete) >= 2:\n",
    "#     doc_complete.pop(1)\n",
    "\n",
    "# Define stopwords and lemmatizer\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess a document\n",
    "def preprocess_document(document):\n",
    "    # Split the document into sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    # Preprocess each sentence\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        \n",
    "        # Remove stopwords and punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha() and token not in stopwords]\n",
    "        \n",
    "        # Lemmatize the tokens\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        preprocessed_sentences.append(tokens)\n",
    "    \n",
    "    return preprocessed_sentences\n",
    "\n",
    "# Create a list of preprocessed documents\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in doc_complete]\n",
    "\n",
    "# Flatten the list of tokens\n",
    "flattened_documents = [\" \".join(token for sentence in document for token in sentence) for document in preprocessed_documents]\n",
    "\n",
    "# Vectorize the documents (TF-IDF)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_docs = vectorizer.fit_transform(flattened_documents)\n",
    "\n",
    "# Perform clustering (K-means)\n",
    "num_clusters = 2 # Adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=40)\n",
    "kmeans.fit(vectorized_docs)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Generate summaries for each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_docs = [flattened_documents[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    \n",
    "    # Calculate the centroid of the cluster\n",
    "    cluster_vectors = vectorized_docs[cluster_labels == cluster_id]\n",
    "    centroid = cluster_vectors.mean(axis=0)\n",
    "    \n",
    "    # Calculate cosine distances between documents and the centroid\n",
    "    distances = cosine_distances(centroid, cluster_vectors)[0]\n",
    "    \n",
    "    # Sort documents by distance\n",
    "    sorted_indices = sorted(range(len(distances)), key=lambda i: distances[i])\n",
    "    print(sorted_indices)\n",
    "    \n",
    "    # Select top documents for summary\n",
    "    top_doc_indices = sorted_indices[:2]  # Adjust the number of documents for summary\n",
    "    \n",
    "    # Extract top sentences from the selected documents\n",
    "    top_sentences = []\n",
    "    for doc_index in top_doc_indices:\n",
    "        sentences = sent_tokenize(cluster_docs[doc_index])\n",
    "        top_sentences.extend(sentences[:5])  # Adjust the number of sentences per document\n",
    "    \n",
    "    # Generate summary \n",
    "    summary = \". \".join(top_sentences)\n",
    "    summarized_text = summarize(summary, words=200)  # Adjust the number of words in the summary\n",
    "    \n",
    "    # Print the summary for the cluster\n",
    "    print(f\"Cluster {cluster_id + 1} Summary:\")\n",
    "    print(summarized_text)\n",
    "    print(\"==========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e149f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Cluster 1 Summary:\n",
      "\n",
      "==========================\n",
      "[2, 0, 1]\n",
      "Cluster 2 Summary:\n",
      "union agriculture minister narendra singh tomar friday said government making effort boost production consumption millet speaking inaugurating millet experience centre mec dilli haat ina national capital cooperative nafed collaboration agriculture ministry established centre create awareness benefit millet encourage adoption among general public agriculture secretary manoj ahuja nafed md rajbir singh present event making effort enhance production yield processing consumption miller country tomar said minister highlighted multiple health benefit consuming millet nutritious tomar said millet climate resilient grown le water minimal use fertiliser pesticide said increase production millet also boost income farmer especially small marginal one tomar noted large number startup commendable job field millet rued importance millet plate food got reduced year acting upon india proposal supported country united nation general assembly declared year international year millet iym declaration positioned india forefront celebration government working mission mode champion millet crop good farmer environment consumer tomar highlighted yearlong celebration millet shree anna entail myriad activity aimed creating awareness around environmental health economic benefit millet farming ministryled initiative establishing consumeroriented millet experience centre would promote dietary benefit ancient grain also popularise millet shree anna nutritional powerhouse fit cooking variety dish like millet dosa millet pasta etc addition unique dining experience customer also purchase variety readytoeat readytocook product local millet startup experience centre ahuja highlighted governmentled initiative mainstreaming millet speaking collaboration nafed ministry said venture like millet experience centre would help widen horizon consumer actively looking healthier alternative bring visibility india robust milletbased startup community nafed md rajbir singh said centre unique concept help recognition immense potential millet shree anna versatile healthy grain added centre enable consumer enjoy expansive millet menu instore shopping experience featuring wide variety milletbased product developed homegrown startup one roof singh said milletsbased product also instrumental promoting healthy snacking among customer encouraging towards adopting healthier milletscentric diet india produce lakh tonne millet jowar bajra ragi sawan kangni cheena kodo kutki kuttu major millet.\n",
      "==========================\n",
      "[0]\n",
      "Cluster 3 Summary:\n",
      "\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Preprocess the text data\n",
    "\n",
    "# if len(doc_complete) >= 2:\n",
    "#     doc_complete.pop(1)\n",
    "\n",
    "# Define stopwords and lemmatizer\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess a document\n",
    "def preprocess_document(document):\n",
    "    # Split the document into sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    # Preprocess each sentence\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        \n",
    "        # Remove stopwords and punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha() and token not in stopwords]\n",
    "        \n",
    "        # Lemmatize the tokens\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        preprocessed_sentences.append(tokens)\n",
    "    \n",
    "    return preprocessed_sentences\n",
    "\n",
    "# Create a list of preprocessed documents\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in doc_complete]\n",
    "\n",
    "# Flatten the list of tokens\n",
    "flattened_documents = [\" \".join(token for sentence in document for token in sentence) for document in preprocessed_documents]\n",
    "\n",
    "# Vectorize the documents (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.1)  # Adjust the values as needed\n",
    "vectorized_docs = vectorizer.fit_transform(flattened_documents)\n",
    "\n",
    "# Perform clustering (K-means)\n",
    "num_clusters = 3  # Adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit(vectorized_docs)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Generate summaries for each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_docs = [flattened_documents[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    \n",
    "    # Calculate the centroid of the cluster\n",
    "    cluster_vectors = vectorized_docs[cluster_labels == cluster_id]\n",
    "    centroid = cluster_vectors.mean(axis=0)\n",
    "    \n",
    "    # Calculate cosine distances between documents and the centroid\n",
    "    distances = cosine_distances(centroid, cluster_vectors)[0]\n",
    "    \n",
    "    # Sort documents by distance\n",
    "    sorted_indices = sorted(range(len(distances)), key=lambda i: distances[i])\n",
    "    \n",
    "    print(sorted_indices)\n",
    "    \n",
    "    # Select top documents for summary\n",
    "    top_doc_indices = sorted_indices[:5]  # Adjust the number of documents for summary\n",
    "    \n",
    "    # Extract top sentences from the selected documents\n",
    "    top_sentences = []\n",
    "    for doc_index in top_doc_indices:\n",
    "        sentences = sent_tokenize(cluster_docs[doc_index])\n",
    "        top_sentences.extend(sentences[:3])  # Adjust the number of sentences per document\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = \". \".join(top_sentences)\n",
    "    summarized_text = summarize(summary, words=200)  # Adjust the number of words in the summary\n",
    "    \n",
    "    # Print the summary for the cluster\n",
    "    print(f\"Cluster {cluster_id + 1} Summary:\")\n",
    "    print(summarized_text)\n",
    "    print(\"==========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcd14825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(doc_complete[0])\n",
    "sentences = doc_complete[0].split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "854b3355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24d44aa0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Government of India to encourage millets cultivation and consumption declared 2018 as the national year of millets. While this is already a vast number of people proponents of millet production are of the opinion that a greater number of people should include millet in their diet. Millets have been a staple of the Indian diet especially in rural India for years and remain prevalent even today\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "The delegates will be taken to Sarnath on April 18. The theme of the meeting is Sustainable Agriculture and Food Systems for Healthy People and the Planet. Thereafter a welcome dinner and cultural programme is organised at Taj Ganges\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "In addition to a unique dining experience customers can also purchase a variety of readytoeat and readytocook products from local millet startups at the experience centre. Union Agriculture Minister Narendra Singh Tomar on Friday said the government is making efforts to boost the production and consumption of millets. Nafed MD Rajbir Singh said the centre is a unique concept that will help in the recognition of the immense potential of millets Shree Anna as a versatile and healthy grain\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "FSSAI has referred the matter to licensing authorities concerned to issue notices to these FBOs for withdrawing misleading claims or scientifically substantiate claims. In order to keep a close tab on the claims and advertisements being made by the FBOs on their products Advertisement Monitoring Committee of FSSAI has reported 32 fresh cases which have been found prima facie in contravention of the provisions of Food Safety and Standards Advertisements  Claims Regulations 2018 the regulator said in a statement. These FBOs include manufacturers andor marketers of nutraceutical products refined oils pulses flours millet products ghee etc\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "The business requires a new set of skills distribution product development sales and marketing Karthik Jayaraman Managing Director WayCool Foods told reporters here on Friday. The Rs 1800crore revenuefarmtodiningplate agri commerce company WayCool Foods and Products has hived off its three brands into a wholly owned subsidiary in order to double to revenue in a year said a top company official. The new company WayCool BrandsNext Private Ltd will soon launch milletbased products and various rice varieties\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Convert text to lowercase\n",
    "#     text = text.lower()\n",
    "\n",
    "#     # Remove punctuation\n",
    "#     text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "#     return text\n",
    "\n",
    "def clustering_summarization(text, num_clusters, num_sentences):\n",
    "    # Preprocess the text\n",
    "#     text = preprocess_text(text)\n",
    "\n",
    "    # Split the text into sentences using the delimiter \".\"\n",
    "    sentences = text.split(\".\")\n",
    "\n",
    "    # Remove empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip() != \"\"]\n",
    "\n",
    "    # Check the number of sentences\n",
    "    if len(sentences) < num_clusters:\n",
    "        num_clusters = len(sentences)\n",
    "\n",
    "    # Vectorize the sentences using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(sentence_vectors)\n",
    "\n",
    "    # Initialize cluster representatives\n",
    "    cluster_representatives = []\n",
    "\n",
    "    # Select representative sentences from each cluster\n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_sentences = [sentence for i, sentence in enumerate(sentences) if cluster_labels[i] == cluster_id]\n",
    "        cluster_sentence_vectors = sentence_vectors[cluster_labels == cluster_id]\n",
    "\n",
    "        # Calculate the centroid vector for the cluster\n",
    "        centroid = np.mean(cluster_sentence_vectors, axis=0)\n",
    "\n",
    "        # Calculate the cosine distances between each sentence vector in the cluster and the centroid\n",
    "        distances = cosine_distances(cluster_sentence_vectors, centroid)\n",
    "\n",
    "        # Sort sentences based on their distances to the centroid\n",
    "        sorted_sentences = sorted(enumerate(cluster_sentences), key=lambda x: distances[x[0]], reverse=False)\n",
    "\n",
    "        # Select the top representative sentence from the cluster\n",
    "        representative_sentence = sorted_sentences[0][1]\n",
    "        cluster_representatives.append(representative_sentence)\n",
    "\n",
    "    # Select top 'num_sentences' representatives as the summary\n",
    "    summary = cluster_representatives[:num_sentences]\n",
    "\n",
    "    # Join the summary sentences into a single string\n",
    "    summary_text = '. '.join(summary)\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "\n",
    "num_clusters = 3\n",
    "num_sentences = 3\n",
    "\n",
    "text = doc_complete[0]  # Select the first document from the list\n",
    "\n",
    "for doc in doc_complete:\n",
    "\n",
    "    summary = clustering_summarization(doc, num_clusters, num_sentences)\n",
    "    print(summary)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
