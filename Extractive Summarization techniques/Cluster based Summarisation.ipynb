{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03556489",
   "metadata": {},
   "source": [
    "# Cluster based Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01b9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "corpus_path = \"news-corpus//\"\n",
    "article_paths = [os.path.join(corpus_path,p) for p in os.listdir(corpus_path)]\n",
    "\n",
    "doc_complete = []\n",
    "for path in article_paths:\n",
    "    with open(path, 'rb') as f:\n",
    "        doc_content = f.read().decode(errors='ignore')\n",
    "        doc_complete.append(doc_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68bcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(doc_complete)):\n",
    "    doc_complete[i] = re.sub(r'[^\\w\\s.]', '', doc_complete[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943876be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(doc_complete) >= 2:\n",
    "    doc_complete.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d8811a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d44e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236c5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1]\n",
      "Cluster 1 Summary:\n",
      "union agriculture minister narendra singh tomar friday said government making effort boost production consumption millet speaking inaugurating millet experience centre mec dilli haat ina national capital cooperative nafed collaboration agriculture ministry established centre create awareness benefit millet encourage adoption among general public agriculture secretary manoj ahuja nafed md rajbir singh present event making effort enhance production yield processing consumption miller country tomar said minister highlighted multiple health benefit consuming millet nutritious tomar said millet climate resilient grown le water minimal use fertiliser pesticide said increase production millet also boost income farmer especially small marginal one tomar noted large number startup commendable job field millet rued importance millet plate food got reduced year acting upon india proposal supported country united nation general assembly declared year international year millet iym declaration positioned india forefront celebration government working mission mode champion millet crop good farmer environment consumer tomar highlighted yearlong celebration millet shree anna entail myriad activity aimed creating awareness around environmental health economic benefit millet farming ministryled initiative establishing consumeroriented millet experience centre would promote dietary benefit ancient grain also popularise millet shree anna nutritional powerhouse fit cooking variety dish like millet dosa millet pasta etc addition unique dining experience customer also purchase variety readytoeat readytocook product local millet startup experience centre ahuja highlighted governmentled initiative mainstreaming millet speaking collaboration nafed ministry said venture like millet experience centre would help widen horizon consumer actively looking healthier alternative bring visibility india robust milletbased startup community nafed md rajbir singh said centre unique concept help recognition immense potential millet shree anna versatile healthy grain added centre enable consumer enjoy expansive millet menu instore shopping experience featuring wide variety milletbased product developed homegrown startup one roof singh said milletsbased product also instrumental promoting healthy snacking among customer encouraging towards adopting healthier milletscentric diet india produce lakh tonne millet jowar bajra ragi sawan kangni cheena kodo kutki kuttu major millet.\n",
      "==========================\n",
      "[0, 1]\n",
      "Cluster 2 Summary:\n",
      "food regulator fssai friday said new case food business operator fbos primafacie found violation misleading advertisement claim fssai referred matter licensing authority concerned issue notice fbos withdrawing misleading claim scientifically substantiate claim fbos include manufacturer andor marketer nutraceutical product refined oil pulse flour millet product ghee etc order keep close tab claim advertisement made fbos product advertisement monitoring committee fssai reported fresh case found prima facie contravention provision food safety standard advertisement claim regulation regulator said statement regulation deceptive claim advertisement prohibited punishable offence f act food product scrutinised include various range product like health supplement organic product fastmoving consumer good fmcg product staple etc claim identified include various health claim product claim etc action including issuance notice concerned fbos referred concerned licensing authority issuance notice fbos withdrawing misleading claim scientifically substantiate statement said case unsatisfactory response fbo required withdraw claim modify per provision said regulation failing fbo penalised fine extending r lakh per food safety standard act apart stringent punishment like suspensioncancellation license etc case repeated offence total number case reported misleading advertisement claim last six month gone case action delinquent fbos shall also continue future fssai said regulator asked fbos strictly adhere provision food safety standard advertisement claim regulation desist making unscientific andor exaggerated claim advertisement promote product sale avoid enforcement action.\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Preprocess the text data\n",
    "\n",
    "# if len(doc_complete) >= 2:\n",
    "#     doc_complete.pop(1)\n",
    "\n",
    "# Define stopwords and lemmatizer\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess a document\n",
    "def preprocess_document(document):\n",
    "    # Split the document into sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    # Preprocess each sentence\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        \n",
    "        # Remove stopwords and punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha() and token not in stopwords]\n",
    "        \n",
    "        # Lemmatize the tokens\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        preprocessed_sentences.append(tokens)\n",
    "    \n",
    "    return preprocessed_sentences\n",
    "\n",
    "# Create a list of preprocessed documents\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in doc_complete]\n",
    "\n",
    "# Flatten the list of tokens\n",
    "flattened_documents = [\" \".join(token for sentence in document for token in sentence) for document in preprocessed_documents]\n",
    "\n",
    "# Vectorize the documents (TF-IDF)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_docs = vectorizer.fit_transform(flattened_documents)\n",
    "\n",
    "# Perform clustering (K-means)\n",
    "num_clusters = 2 # Adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=40)\n",
    "kmeans.fit(vectorized_docs)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Generate summaries for each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_docs = [flattened_documents[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    \n",
    "    # Calculate the centroid of the cluster\n",
    "    cluster_vectors = vectorized_docs[cluster_labels == cluster_id]\n",
    "    centroid = cluster_vectors.mean(axis=0)\n",
    "    \n",
    "    # Calculate cosine distances between documents and the centroid\n",
    "    distances = cosine_distances(centroid, cluster_vectors)[0]\n",
    "    \n",
    "    # Sort documents by distance\n",
    "    sorted_indices = sorted(range(len(distances)), key=lambda i: distances[i])\n",
    "    print(sorted_indices)\n",
    "    \n",
    "    # Select top documents for summary\n",
    "    top_doc_indices = sorted_indices[:2]  # Adjust the number of documents for summary\n",
    "    \n",
    "    # Extract top sentences from the selected documents\n",
    "    top_sentences = []\n",
    "    for doc_index in top_doc_indices:\n",
    "        sentences = sent_tokenize(cluster_docs[doc_index])\n",
    "        top_sentences.extend(sentences[:5])  # Adjust the number of sentences per document\n",
    "    \n",
    "    # Generate summary \n",
    "    summary = \". \".join(top_sentences)\n",
    "    summarized_text = summarize(summary, words=200)  # Adjust the number of words in the summary\n",
    "    \n",
    "    # Print the summary for the cluster\n",
    "    print(f\"Cluster {cluster_id + 1} Summary:\")\n",
    "    print(summarized_text)\n",
    "    print(\"==========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e149f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Cluster 1 Summary:\n",
      "\n",
      "==========================\n",
      "[2, 0, 1]\n",
      "Cluster 2 Summary:\n",
      "union agriculture minister narendra singh tomar friday said government making effort boost production consumption millet speaking inaugurating millet experience centre mec dilli haat ina national capital cooperative nafed collaboration agriculture ministry established centre create awareness benefit millet encourage adoption among general public agriculture secretary manoj ahuja nafed md rajbir singh present event making effort enhance production yield processing consumption miller country tomar said minister highlighted multiple health benefit consuming millet nutritious tomar said millet climate resilient grown le water minimal use fertiliser pesticide said increase production millet also boost income farmer especially small marginal one tomar noted large number startup commendable job field millet rued importance millet plate food got reduced year acting upon india proposal supported country united nation general assembly declared year international year millet iym declaration positioned india forefront celebration government working mission mode champion millet crop good farmer environment consumer tomar highlighted yearlong celebration millet shree anna entail myriad activity aimed creating awareness around environmental health economic benefit millet farming ministryled initiative establishing consumeroriented millet experience centre would promote dietary benefit ancient grain also popularise millet shree anna nutritional powerhouse fit cooking variety dish like millet dosa millet pasta etc addition unique dining experience customer also purchase variety readytoeat readytocook product local millet startup experience centre ahuja highlighted governmentled initiative mainstreaming millet speaking collaboration nafed ministry said venture like millet experience centre would help widen horizon consumer actively looking healthier alternative bring visibility india robust milletbased startup community nafed md rajbir singh said centre unique concept help recognition immense potential millet shree anna versatile healthy grain added centre enable consumer enjoy expansive millet menu instore shopping experience featuring wide variety milletbased product developed homegrown startup one roof singh said milletsbased product also instrumental promoting healthy snacking among customer encouraging towards adopting healthier milletscentric diet india produce lakh tonne millet jowar bajra ragi sawan kangni cheena kodo kutki kuttu major millet.\n",
      "==========================\n",
      "[0]\n",
      "Cluster 3 Summary:\n",
      "\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/kusumachalla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Preprocess the text data\n",
    "\n",
    "# if len(doc_complete) >= 2:\n",
    "#     doc_complete.pop(1)\n",
    "\n",
    "# Define stopwords and lemmatizer\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess a document\n",
    "def preprocess_document(document):\n",
    "    # Split the document into sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    # Preprocess each sentence\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        \n",
    "        # Remove stopwords and punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha() and token not in stopwords]\n",
    "        \n",
    "        # Lemmatize the tokens\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        preprocessed_sentences.append(tokens)\n",
    "    \n",
    "    return preprocessed_sentences\n",
    "\n",
    "# Create a list of preprocessed documents\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in doc_complete]\n",
    "\n",
    "# Flatten the list of tokens\n",
    "flattened_documents = [\" \".join(token for sentence in document for token in sentence) for document in preprocessed_documents]\n",
    "\n",
    "# Vectorize the documents (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.1)  # Adjust the values as needed\n",
    "vectorized_docs = vectorizer.fit_transform(flattened_documents)\n",
    "\n",
    "# Perform clustering (K-means)\n",
    "num_clusters = 3  # Adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit(vectorized_docs)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Generate summaries for each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_docs = [flattened_documents[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    \n",
    "    # Calculate the centroid of the cluster\n",
    "    cluster_vectors = vectorized_docs[cluster_labels == cluster_id]\n",
    "    centroid = cluster_vectors.mean(axis=0)\n",
    "    \n",
    "    # Calculate cosine distances between documents and the centroid\n",
    "    distances = cosine_distances(centroid, cluster_vectors)[0]\n",
    "    \n",
    "    # Sort documents by distance\n",
    "    sorted_indices = sorted(range(len(distances)), key=lambda i: distances[i])\n",
    "    \n",
    "    print(sorted_indices)\n",
    "    \n",
    "    # Select top documents for summary\n",
    "    top_doc_indices = sorted_indices[:5]  # Adjust the number of documents for summary\n",
    "    \n",
    "    # Extract top sentences from the selected documents\n",
    "    top_sentences = []\n",
    "    for doc_index in top_doc_indices:\n",
    "        sentences = sent_tokenize(cluster_docs[doc_index])\n",
    "        top_sentences.extend(sentences[:3])  # Adjust the number of sentences per document\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = \". \".join(top_sentences)\n",
    "    summarized_text = summarize(summary, words=200)  # Adjust the number of words in the summary\n",
    "    \n",
    "    # Print the summary for the cluster\n",
    "    print(f\"Cluster {cluster_id + 1} Summary:\")\n",
    "    print(summarized_text)\n",
    "    print(\"==========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcd14825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(doc_complete[0])\n",
    "sentences = doc_complete[0].split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "854b3355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24d44aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Government of India to encourage millets cultivation and consumption declared 2018 as the national year of millets. While this is already a vast number of people proponents of millet production are of the opinion that a greater number of people should include millet in their diet. We are the largest producer and the second largest exporter of Shri Anna Millets in the world. Millets have been a staple of the Indian diet especially in rural India for years and remain prevalent even today\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "The delegates will be taken to Sarnath on April 18. The district administration has also made elaborate arrangements for the delegates to facilitate their convenient movement in the city. The theme of the meeting is Sustainable Agriculture and Food Systems for Healthy People and the Planet. Thereafter a welcome dinner and cultural programme is organised at Taj Ganges\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "The ministryled initiative of establishing a consumeroriented Millets Experience Centre would not only promote the dietary benefits of the ancient grain but also popularise millets or Shree Anna as a nutritional powerhouse fit for cooking a variety of dishes like Millets dosa Millets pasta etc. In addition to a unique dining experience customers can also purchase a variety of readytoeat and readytocook products from local millet startups at the experience centre. Tomar noted that a large number of startups are doing a commendable job in the field of millet. Union Agriculture Minister Narendra Singh Tomar on Friday said the government is making efforts to boost the production and consumption of millets\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "FSSAI has referred the matter to licensing authorities concerned to issue notices to these FBOs for withdrawing misleading claims or scientifically substantiate claims. Under these regulations deceptive claims or advertisements are prohibited and are punishable offences under Section53 of the FSS Act 2006. In order to keep a close tab on the claims and advertisements being made by the FBOs on their products Advertisement Monitoring Committee of FSSAI has reported 32 fresh cases which have been found prima facie in contravention of the provisions of Food Safety and Standards Advertisements  Claims Regulations 2018 the regulator said in a statement. These FBOs include manufacturers andor marketers of nutraceutical products refined oils pulses flours millet products ghee etc\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Last year the revenue from the hived off brands  Kitchenji dals staples and spices Madhuram bulk grain and pulse Fresheys readytocook items dairy products  was about Rs 400 crore. WayCool Indias leading food and agritech company has signed a Memorandum of Understanding MoU with Central Warehousing Corporation CWC to bolster the distribution of its agriinput portfolio. The new company WayCool BrandsNext Private Ltd will soon launch milletbased products and various rice varieties. According to Jayaraman the demand for whitelabelled products have gone up and the company is also serving the needs of companies that are into delivering groceries to homes\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Convert text to lowercase\n",
    "#     text = text.lower()\n",
    "\n",
    "#     # Remove punctuation\n",
    "#     text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "#     return text\n",
    "\n",
    "def clustering_summarization(text, num_clusters, num_sentences):\n",
    "    # Preprocess the text\n",
    "#     text = preprocess_text(text)\n",
    "\n",
    "    # Split the text into sentences using the delimiter \".\"\n",
    "    sentences = text.split(\".\")\n",
    "\n",
    "    # Remove empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip() != \"\"]\n",
    "\n",
    "    # Check the number of sentences\n",
    "    if len(sentences) < num_clusters:\n",
    "        num_clusters = len(sentences)\n",
    "\n",
    "    # Vectorize the sentences using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(sentence_vectors)\n",
    "\n",
    "    # Initialize cluster representatives\n",
    "    cluster_representatives = []\n",
    "\n",
    "    # Select representative sentences from each cluster\n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_sentences = [sentence for i, sentence in enumerate(sentences) if cluster_labels[i] == cluster_id]\n",
    "        cluster_sentence_vectors = sentence_vectors[cluster_labels == cluster_id]\n",
    "\n",
    "        # Calculate the centroid vector for the cluster\n",
    "        centroid = np.mean(cluster_sentence_vectors, axis=0)\n",
    "\n",
    "        # Calculate the cosine distances between each sentence vector in the cluster and the centroid\n",
    "        distances = cosine_distances(cluster_sentence_vectors, centroid)\n",
    "\n",
    "        # Sort sentences based on their distances to the centroid\n",
    "        sorted_sentences = sorted(enumerate(cluster_sentences), key=lambda x: distances[x[0]], reverse=False)\n",
    "\n",
    "        # Select the top representative sentence from the cluster\n",
    "        representative_sentence = sorted_sentences[0][1]\n",
    "        cluster_representatives.append(representative_sentence)\n",
    "\n",
    "    # Select top 'num_sentences' representatives as the summary\n",
    "    summary = cluster_representatives[:num_sentences]\n",
    "\n",
    "    # Join the summary sentences into a single string\n",
    "    summary_text = '. '.join(summary)\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "generated_summaries = []\n",
    "num_clusters = 4\n",
    "num_sentences = 4\n",
    "\n",
    "text = doc_complete[0]  # Select the first document from the list\n",
    "\n",
    "for doc in doc_complete:\n",
    "\n",
    "    summary = clustering_summarization(doc, num_clusters, num_sentences)\n",
    "    print(summary)\n",
    "    generated_summaries.append(summary)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afca6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_between_markers(text, start_marker, end_marker):\n",
    "    extracted_texts = []\n",
    "    start_index = 0\n",
    "\n",
    "    while True:\n",
    "        start_index = text.find(start_marker, start_index)\n",
    "        if start_index == -1:\n",
    "            break\n",
    "\n",
    "        start_index += len(start_marker)\n",
    "        end_index = text.find(end_marker, start_index)\n",
    "        if end_index == -1:\n",
    "            break\n",
    "\n",
    "        extracted_text = text[start_index:end_index].strip()\n",
    "        extracted_texts.append(extracted_text)\n",
    "\n",
    "        start_index = end_index + len(end_marker)\n",
    "\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab61443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "reference_summaries =[]\n",
    "file_path = \"News articles.docx\"\n",
    "text_content = read_docx(file_path)\n",
    "\n",
    "\n",
    "start_marker = \"Summary:\"\n",
    "end_marker = \"Top Sentences:\"\n",
    "\n",
    "extracted_texts = extract_text_between_markers(text_content, start_marker, end_marker)\n",
    "for extracted_text in extracted_texts:\n",
    "    reference_summaries.append(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83dd9164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Indian government, led by Agriculture Minister Narendra Singh Tomar, has opened the 'Millets Experience Centre' in New Delhi to promote the production and consumption of millets. The center aims to raise awareness about the nutritional benefits of millets and encourage their adoption among the public. Tomar emphasized the health advantages of millets and their climate resilience, highlighting that increased millet production would benefit farmers and startups in the sector. The United Nations has declared 2023 as the International Year of Millets, further positioning India as a leader in promoting this crop.\",\n",
       " \"India is taking initiatives to educate people about the nutritional value of millets and promote their consumption. Despite being rich in protein and antioxidants, millets have not been considered fashionable foods. However, the Indian government has recognized millets as a reliable grain for enhancing farmers' income and ensuring food security. Efforts to revive millet production and consumption have led to increased exports and the declaration of 2023 as the international year of millets. India aims to raise awareness about millets' nutritional value and address food inequity through their promotion.\",\n",
       " 'The Food Safety and Standards Authority of India (FSSAI) has identified 32 cases of food business operators (FBOs) violating regulations related to misleading advertisements and claims. These FBOs, including manufacturers and marketers of various products, have been referred to licensing authorities to withdraw the misleading claims or provide scientific substantiation. The FSSAI has taken action against a total of 170 such cases in the past six months and warns FBOs to adhere to regulations to avoid enforcement actions.',\n",
       " \"India's G20 presidency is hosting a three-day Meeting of Agricultural Chief Scientists (MACS) in Varanasi to discuss sustainable agriculture and food systems. Topics such as agriculture research and development, food security, climate smart agriculture, and digital agriculture will be deliberated. The meeting will also focus on the 'Millets And Other Ancient Grains International Research Initiative (MAHARISHI)' as a G20 initiative. Around 80 foreign delegates from G20 member states, along with invited guest countries and international bodies, will attend the meeting. The delegates will also experience India's cultural heritage and visit historical sites during their stay in Varanasi.\",\n",
       " 'Agri-commerce company WayCool Foods and Products has created a wholly owned subsidiary, WayCool BrandsNext Private Ltd, to double its revenue in a year. The subsidiary will launch millet-based products, various rice varieties, and value-added dairy products. WayCool Foods aims to reach a revenue of Rs 800 crore by expanding its reach and launching new products. The company has also partnered with Central Warehousing Corporation to enhance its agri-input distribution and improve supply chain efficiency.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002348be",
   "metadata": {},
   "source": [
    "# Bleu Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9091616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for Article : 8.618388845839317e-232\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_score = sentence_bleu(generated_summaries[0],reference_summaries[0])\n",
    "print(\"BLEU score for Article\",\":\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d55d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for Article : 8.436134752850363e-232\n"
     ]
    }
   ],
   "source": [
    "bleu_score = sentence_bleu(generated_summaries[2],reference_summaries[1])\n",
    "print(\"BLEU score for Article\",\":\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "429c124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for Article : 9.252921945181717e-232\n"
     ]
    }
   ],
   "source": [
    "bleu_score = sentence_bleu(generated_summaries[3],reference_summaries[2])\n",
    "print(\"BLEU score for Article\",\":\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c52fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for Article : 8.24292407145299e-232\n"
     ]
    }
   ],
   "source": [
    "bleu_score = sentence_bleu(generated_summaries[1],reference_summaries[3])\n",
    "print(\"BLEU score for Article\",\":\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "997befb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for Article : 9.338418756557488e-232\n"
     ]
    }
   ],
   "source": [
    "bleu_score = sentence_bleu(generated_summaries[4],reference_summaries[4])\n",
    "print(\"BLEU score for Article\",\":\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
